import os
import warnings
import config
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë‚´ìš©ìœ¼ë¡œ í•„í„°ë§í•˜ì—¬ ìˆ¨ê¹ë‹ˆë‹¤.
warnings.filterwarnings("ignore", message=".*deprecated.*")

# ğŸ”¹ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ğŸ”¹ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
explanation_prompt_template = """
'{question}'ì€(ëŠ”) í‰ê·  ì ìˆ˜ {average_score:.1f}ì ìœ¼ë¡œ '{classification}'ìœ¼ë¡œ íŒë³„ë˜ì—ˆìŠµë‹ˆë‹¤.

ì•„ë˜ ë¦¬ë·° ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ, ì´ íŒë³„ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì´ìœ ë¥¼ ê¸ì •ì , ë¶€ì •ì  ì¸¡ë©´ì—ì„œ ìš”ì•½í•´ ì£¼ì‹­ì‹œì˜¤.
'ë‚´ëˆë‚´ì‚°', 'ì¬ë°©ë¬¸' ë“±ì˜ ì‹ ë¢°ë„ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´ ê°•ì¡°í•´ì„œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

ë¦¬ë·° ìš”ì•½:
{context}

ì¶œë ¥ í˜•ì‹:
'{question}'ëŠ” [{classification}]ìœ¼ë¡œ íŒë³„ë©ë‹ˆë‹¤. (í‰ê·  ì ìˆ˜: {average_score:.1f}ì )
ì´ìœ :
1. ê¸ì •ì  í‘œí˜„ ìš”ì•½:
2. ë¶€ì •ì  í‘œí˜„ ìš”ì•½:
3. ì‹ ë¢°ë„ íŒë‹¨ ê·¼ê±°:
"""

query_expansion_prompt_template = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ë¥¼ í™•ì¥í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ ë†’ì´ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì›ë˜ ê²€ìƒ‰ì–´ì™€ ê´€ë ¨ëœ, ê²€ìƒ‰ì— ë„ì›€ì´ ë  ë§Œí•œ ì¶”ê°€ ê²€ìƒ‰ì–´ 3ê°œë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ìƒì„±í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
- ì…ë ¥: "ë§Œë“ì´ë„¤ ë‘ë£¨ì¹˜ê¸°"
- ì¶œë ¥: "ë§Œë“ì´ë„¤, ë§Œë“ì´ë„¤ í›„ê¸°, ë‚¨ê°€ì¢Œë™ ë§Œë“ì´ë„¤"

- ì…ë ¥: "ìŠ¤íƒ€ë²…ìŠ¤"
- ì¶œë ¥: "ìŠ¤íƒ€ë²…ìŠ¤ í›„ê¸°, ìŠ¤íƒ€ë²…ìŠ¤ ë©”ë‰´, ìŠ¤íƒ€ë²…ìŠ¤ ë¦¬ë·°"

ì´ì œ ë‹¤ìŒ ê²€ìƒ‰ì–´ì— ëŒ€í•œ ì¶”ê°€ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì…ë ¥: "{question}"
ì¶œë ¥:
"""

def run_rag(store_name, vectordb_path=config.VECTORDB_PATH):
    llm = ChatOpenAI(model=config.OPENAI_CHAT_MODEL, openai_api_key=OPENAI_API_KEY, temperature=config.OPENAI_TEMPERATURE)

    # 1. ì¿¼ë¦¬ í™•ì¥
    print(f"ğŸ”„ '{store_name}'ì— ëŒ€í•œ ì¿¼ë¦¬ í™•ì¥ ì¤‘...")
    query_expansion_prompt = PromptTemplate.from_template(query_expansion_prompt_template)
    query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()
    expanded_queries_str = query_expansion_chain.invoke({"question": store_name})
    
    search_queries = [store_name] + [q.strip() for q in expanded_queries_str.split(',')]
    print(f"ğŸ” í™•ì¥ëœ ê²€ìƒ‰ì–´: {search_queries}")

    # 2. í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰ ë° ì·¨í•©
    emb = OpenAIEmbeddings(model=config.OPENAI_EMBEDDING_MODEL, openai_api_key=OPENAI_API_KEY)
    db = Chroma(persist_directory=vectordb_path, embedding_function=emb)
    
    unique_docs = {}
    for query in search_queries:
        retrieved_docs_with_scores = db.similarity_search_with_relevance_scores(
            query=query, 
            k=config.RETRIEVER_SEARCH_K
        )
        for doc, score in retrieved_docs_with_scores:
            if score >= config.SIMILARITY_THRESHOLD:
                # ë¬¸ì„œ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  í‚¤ ìƒì„±
                doc_key = (doc.page_content, doc.metadata.get('score'), doc.metadata.get('label'))
                if doc_key not in unique_docs:
                    unique_docs[doc_key] = doc

    # 3. ì¬ì ìˆ˜í™”(Re-ranking): ì¶”ì¶œëœ ê°€ê²Œì´ë¦„ì´ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
    priority_docs = []
    other_docs = []
    for doc in unique_docs.values():
        # ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ì— ì¶”ì¶œëœ ê°€ê²Œ ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ë¶€ë¶„ ì¼ì¹˜ í—ˆìš©)
        if store_name in doc.metadata.get('extracted_name', ''):
            priority_docs.append(doc)
        else:
            other_docs.append(doc)
    
    relevant_docs = priority_docs + other_docs
    print(f"âœ¨ ì¬ì ìˆ˜í™” ì™„ë£Œ: ìš°ì„ ìˆœìœ„ ë¬¸ì„œ {len(priority_docs)}ê°œ / ì „ì²´ {len(relevant_docs)}ê°œ")

    # 4. ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°, ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜
    if not relevant_docs:
        return f"'{store_name}'ì— ëŒ€í•œ ë¦¬ë·° ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´, ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°€ê²Œ ì´ë¦„ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."

    # --- (ì´í•˜ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼) ---

    # 5. ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì ìˆ˜ ë° ë¦¬ë·° ìˆ˜ ê³„ì‚°
    v = len(relevant_docs)
    total_score = sum(doc.metadata.get('score', 0) for doc in relevant_docs)
    R = total_score / v if v > 0 else 0

    # 6. ê°€ì¤‘ í‰ì (Weighted Rating) ê³„ì‚°
    m = config.MIN_REVIEW_COUNT
    C = config.GLOBAL_AVERAGE_SCORE
    
    average_score = (v / (v + m)) * R + (m / (v + m)) * C

    # 7. ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ 'ë§›ì§‘'/'ë¹„ë§›ì§‘' ë¶„ë¥˜
    classification = "ë§›ì§‘" if average_score >= config.SCORE_THRESHOLD else "ë¹„ë§›ì§‘"

    # 8. LLMì„ í†µí•´ íŒë³„ ì´ìœ  ìƒì„±
    context = "\n\n---\n\n".join([doc.page_content for doc in relevant_docs])
    
    prompt = PromptTemplate(
        template=explanation_prompt_template,
        input_variables=["question", "average_score", "classification", "context"]
    )
    
    chain = prompt | llm
    
    result = chain.invoke({
        "question": store_name,
        "average_score": average_score,
        "classification": classification,
        "context": context
    })

    return result.content

if __name__ == "__main__":
    store = input("ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ë§›ì§‘ íŒë³„ AIğŸšì…ë‹ˆë‹¤. íŒë³„í•˜ê³ ìí•˜ëŠ” ê°€ê²Œì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n> ")
    print("\nğŸš ë§›ì§‘ íŒë³„ ì¤‘...\n")
    print(run_rag(store))